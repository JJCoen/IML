---
title: "Logistic-Regression"
type: inverse
author: "Jim Coen"
date: "`r format(Sys.Date(), '%A, %B %d, %Y') `"
geometry: "left=3cm,right=3cm,top=2cm,bottom=2cm"
output: workflowr::wflow_html
bibliography: C:/Users/jimco/Documents/Mendeley-Bib/Financial-Analytics-Thesis.bib
csl: Harvard.csl
editor_options:
  chunk_output_type: console
---

```{r}
# make this an external chunk that can be included in any file
library(knitr)
options(width = 100)
knitr::opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(digits = 3)
options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```


```{r, load-libraries, echo=FALSE}

# Packages
library(tidyverse)  # data manipulation and visualization
library(tidymodels) # data split
library(modelr)     # provides easy pipeline modeling functions
library(broom)      # helps to tidy up model outputs
library(ggplot2)
library(dplyr)
library(purrr)
library(data.table)
library(kableExtra)

```

Pre-processing has already been performed in *HELOC-data-cleaning.Rmd*

```{r, load_data}
load("data/train_test_imp")
# Set the reference level
train_imp[, RiskPerformance := relevel(RiskPerformance, ref = "Good")]
test_imp[, RiskPerformance := relevel(RiskPerformance, ref = "Good")]
```

## Logistic Regression

The BoE QII model uses log(odds) and probability of default as metrics in graphs that enable interpretation [@Bracke2019]. Therefore, it is informative to review these concepts.

All these models are binary classifiers where the outcome is in one of two classes and can be represented by a binomial function with probability of success $p_i$.

$$ y_i = \mathbf{Binom}(p_i)$$ 
$y_i$ is the **Risk** variable and it takes the values 0 or 1. Generally in credit management, **Risk** = 1 signifies default on a loan, while **Risk** = 0 is non-default.

In order to model the function $y$, linear logistic regression uses a linear combination of the features:

$$
\eta = \beta_0 + \beta_1 x_1 + \beta_2 x_2 \quad . . . 
$$

$\eta$ has values $(-\infty,\infty)$.

Consider the Odds, which is the probability of default divided by the probability of non-default.

$$
\frac{\rm{Pr}(\mathbf{Risk} = 1 | x_1, x_2, \beta_0, \beta_1, \beta_2 )}{1-\rm{Pr}(\mathbf{Risk} = 1 | x_1, x_2, \beta_0, \beta_1, \beta_2 )}
$$

This has values in the range $(0,\infty)$

The logit function, which is the log (Odds), has values $(-\infty,\infty)$. Now it is possible to relate the logit function (log odds of success) to the linear combination of the predictors.

$$
log(\frac{p}{1-p}) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 \quad . . . 
$$

So the Odds in favor of default are:

$$
\frac{p}{1-p} = e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2}  
$$

The inverse logit express the probability $p$ in terms of the sigmoid (logistic) function:

$$
p = \frac{\exp(\eta)}{1 + \exp(\eta)} \text{  where  } \eta = \beta_0 + \beta_1 x_1 + \beta_2 x_2 \quad . . . 
$$

In the Bracke (2019) study, the baseline model is Logistic Regression. For simplicity, consider a logistic regression model based on the first two features in the HELOC data-set.

```{r, echo=TRUE}
set.seed(17032022)
LR_two <- glm(RiskPerformance ~ ExternalRiskEstimate +  MSinceOldestTradeOpen,
              family = "binomial", 
              data = train_imp)
tidy(LR_two)
```

The coefficient estimates given by the model leads to

$$
\begin{align*}
log(\frac{p}{1-p}) &= 8.23 - 0.105 x_1 -0.003 x_2 \quad \text{and} \\
\frac{p}{1-p} &= e^{8.23 - 0.105 x_1 -0.003 x_2}
\end{align*}
$$

### **Interpretation of coefficient**

Let's assume that the value for variable $x_j$ goes up by one, while all other variables remain equal. If this happens, the odds for default will be multiplied by the exponential of $beta_j$.

Note that odds will DECREASE for increasing $x_j$ when $\beta_j$ is negative, and odds will INCREASE for increasing $x_j$ when \$\beta\_j\$ is positive.

-   $\beta_j < 0 \Rightarrow \exp^{\beta_j} < 1$

-   $\beta_j > 0 \Rightarrow \exp^{\beta_j} > 1$

In the logistic regression model above, the coefficient for `ExternalRiskEstimate` is -0.105. If the `ExternalRiskEstimate` increases by one unit, then the Odds for default is multiplied by 0.9 ($e^{-0.105}$). This means that, for each unit increase in `ExternalRiskEstimate`, then the Odds for default lowers by 10%.

```{r}
options(digits = 3)
exp(LR_two$coeff)
```

### Logistic Regression Plot

```{r}
train_sub <- train_imp[, 1:3]
train_sub[, RiskBin := ifelse(RiskPerformance=="Bad", 1, 0)]

ggplot(data= train_sub, mapping = aes(x=ExternalRiskEstimate, y=RiskBin)) + geom_point(alpha=0.5, color = "darkblue") +
  stat_smooth(method="glm", se=TRUE, method.args = list(family=binomial),
              formula = y ~ x,
              col="red", lty=2) + 
  theme(legend.position = c(0.8, 0.8))
```

## References

\bibliography
